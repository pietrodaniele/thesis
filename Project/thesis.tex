\documentclass[a4paper, oneside, 11pt, openright]{book}


\usepackage[a-1b]{pdfx}
\usepackage{cite}
\usepackage{geometry,url,graphicx, hyperref,subfig,enumitem, amsmath,float}
\usepackage{amsmath, amssymb, amsthm, mathtools, color, setspace}
\usepackage{fancyhdr, braket, etoolbox,booktabs,multirow}
\usepackage{xfrac, lmodern, ifsym, bm, multicol, booktabs, pdflscape} % xfrac gives font errors, add lmodern to remove them. Check whether this re, mains a problem!
%\usepackage{hyperref}
%\usepackage[utf8]{inputenc}
%\usepackage{colorprofiles}
\usepackage[T1]{fontenc}

\usepackage{tikz-cd}

\usepackage{stackengine}
\def\tanslant{.25}
\newcommand\hatsrm[2][2]{\ensurestackMath{%
		\ifnum#1>1%
		\stackengine{-4pt}{\hatsrm[\numexpr#1-1\relax]{#2}}{%
			\scriptstyle\char'136}{O}{c}{F}{T}{S}%
		\else%
		\stackengine{-3pt}{#2}{\scriptstyle\char'136}{O}{c}{F}{T}{S}%
		\fi%
}}
\newcommand\hatsit[2][2]{\ensurestackMath{%
		\sbox0{$#2$}%
		\ifnum#1>1%
		\stackengine{-4pt}{\hatsit[\numexpr#1-1\relax]{#2}}{%
			\kern\tanslant\ht0\scriptstyle\char'136}{O}{c}{F}{T}{S}%
		\else%
		\stackengine{-3pt}{#2}{\kern\tanslant\ht0\scriptstyle\char'136}%
		{O}{c}{F}{T}{S}%
		\fi%
}}



\begin{document}
	\begin{titlepage}
		%%%% LOGO %%%%
		\begin{figure}
			\includegraphics[width=\linewidth]{thesis_images/logo.jpg}
		\end{figure}
		\begin{center}
			\textsc{\large Corso di laurea in Fisica}\\[0.2cm]
			\textsc{\normalsize Tesi di laurea magistrale}\\[2cm]
			%\rule{\linewidth}{0.3mm} \\[1.2cm]
			
			% TITLE
			\begin{doublespace}
				\textbf{\LARGE Search for resonances in the 105 to 200 GeV diphoton invariant mass range using 140 fb$^{-1}$ of $pp$ collisions collected at $\sqrt{s}$=13 TeV with the ATLAS detector}
				\\[2cm]
			\end{doublespace}
			
			% AUTHOR
			\begin{minipage}{0.4\textwidth}
				\begin{flushleft}
					\emph{Autore:} \\[0mm]
					\textbf{Pietro Daniele} \\[4mm]
					\emph{Matricola:}\\
					982369 \\[4mm]
					\emph{Codice P.A.C.S.:}\\[0mm]
					12.60.Fr
				\end{flushleft}
			\end{minipage}
			%
			\begin{minipage}{0.4\textwidth}
				\begin{flushright} 
					\emph{Relatore:} \\
					\textbf{Prof. Leonardo Carlo Carminati} \\[1.2em]
					\emph{Corelatori:} \\
					\textbf{Dott. Ruggero Turra} \\
					\textbf{Dott.ssa Elena Mazzeo} \\[1.2em]
				\end{flushright}
			\end{minipage}\\[2cm]
			\vfill
			Anno accademico 2021-2022
		\end{center}
		
		
	\end{titlepage}
	
	\clearpage\null\thispagestyle{empty}\newpage
	
	\tableofcontents
	
	\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}
	
		In 2012, the ATLAS and CMS Collaborations discovered a new boson \cite{higgs_atlas},\cite{higgs_cms} that was consistent with the Higgs boson predicted by the Standard Model of particle physics (SM). Its discovery confirmed the Higgs mechanism for the electroweak symmetry breaking.
		
		While the measurements of the production and decay rates of the 125-GeV Higgs boson ($h_{125}$) are presently consistent with the predictions for the Higgs boson in the Standard Model, there is no requirement for the Higgs sector to be minimal. Extended Higgs sectors can address some of the shortcomings of the SM. For example, the SM provides no dark matter candidate and no explanation for the matter-antimatter asymmetry in the universe, so there must be physics beyond the Standard Model (BSM). On a more theoretical level, %the vast discrepancy between aspects of the weak nuclear force and gravity, 
		the difference in scales between the mass of the Higgs boson and the Planck mass, called hierarchy problem, has puzzled physicists for a long time and continues to do so. Moreover, the SM does not explain its own flavor structure, nor does it yield coupling unification at high energy scales\cite{ext_higgs}.
		
		The Higgs field in the SM forms a complex doublet under the weak isospin SU(2) symmetry group. Larger scalar sectors that include a boson consistent with the $h_{125}$ observation typically incorporate this complex doublet and add additional structure. There can be additional real or complex singlets, doublets, triplets, and forth, or a combination of them. One of such completion could be a two-Higgs-doublet model (2HDM)\cite{Branco_2012}, that gives rise to two neutral and two charged additional Higgs bosons. The extended Higgs sector also can be embedded in a larger theoretical scenario, such as supersymmetry (SUSY) \cite{dine_2016}, or it can be extended to include dark matter candidates. The theoretical approaches can be divided into two categories, bottom-up and top-down approaches. In bottom-up approaches, pure extended scalar sectors are explored in a general fashion and can be considered as low-energy representations of an unknown larger theory at higher scales. In top-down approaches, the extended scalar sector is an intrinsic part of a concrete larger theoretical scenario. The most well-studied full model is the minimal supersymmetric Standard Model (MSSM) \cite{Djouadi_2008}. The MSSM addresses several shortcomings of the SM: it provides a dark matter candidate, allows for the unification of the gauge couplings, and mitigates the hierarchy problem. The MSSM requires a 2HDM-like Higgs sector. More complex models, such as a 2HDM with an additional singlet or a model with an additional triplet, typically reproduce at least part of the phenomenology of singlets and 2HDMs but can add interesting new phenomenology \cite{Dawson_2019}\cite{PhysRevD.98.030001}.
		
		
		
		Extended scalar sectors have two main phenomenological consequences \cite{ext_higgs}\cite{Englert_2014}. Firstly, they lead to additional states, which can come in the form of additional neutral and charged scalars (commonly referred to as additional Higgs bosons). These states can be produced directly at high-energy colliders or leave imprints in precision electroweak measurements. Secondly, extended scalar sectors may modify $h_{125}$ couplings compared with the SM. Therefore, focusing on the first consequence, extended Higgs models give rise to neutral and charged Higgs bosons, which in turn decay into various final states based on their mass and couplings. One of them is the di-photon final state: a search for spin-0 new resonances, in addition to $h_{125}$ Higgs boson, can be performed using the di-photon channel. While the $\gamma\gamma$ branching fraction is subdominant, the searches in this final state profit from an excellent mass resolution and manageable backgrounds. 
		
		Searching for additional Higgs boson from $pp$ collisions in the two photons final states provides a great opportunity to test BSM physics. Therefore, in this thesis, a search for spin-0 new resonances, in addition to the 125-GeV Standard Model (SM) Higgs boson, will be performed in the $\gamma\gamma$ channel in the [105,200] GeV invariant mass range. Searches have been carried out for $m_H$ near 125 GeV in the context of the SM Higgs boson analysis \cite{Lanyov_2014} as well as for higher \cite{2021136651}\cite{2017105} and lower masses (65 < $m_H $ < 110 GeV) \cite{ATLAS-CONF-2018-025}. None of these analyses showed a significant signal excess above the background-only expectations.  The analysis is based on the selection of pairs of high-$p_T$ and isolated photons. The events will be classified into mutually exclusive categories designed to enhance the analysis sensitivity. Different categorisations are tested and compared with each other. The signal in each category is defined as a function of the mass of the resonance and its model is created using simulated MC samples of SM (spin0) Higgs bosons decaying into two photons. The background in each category is described by a smoothly falling function whose normalization and shape parameters will be determined from \textcolor{red}{MC samples}. Since the model must be SM independent in order to investigate the presence of new spin-0 resonances, the SM Higgs$_{125}$ is also included as background. A combined maximum likelihood fit in all the categories is performed to investigate the presence of a signal by computing the compatibility of the observed data with the background-only hypothesis. Looking at limits on the signal cross for each models built with different categorisation, the best working point is selected. \textcolor{red}{Credo di dover aggiungere i risultati brevemente quando finito}
		
		 L section will be placed if no statistically significant excess with respect to the background expectation will be observed.
		
	
	\chapter{LHC and ATLAS Detector}
		\section{LHC}
			The Large Hadron Collider (LHC) \cite{LHC_DESIGN_2004} is a two-ring, superconducting accelerator and collider installed in the 27 km long LEP \cite{LEP_DESIGN_2001} tunnel aiming at the discovery of the Higgs particle and the study of rare events with centre of mass collision energies of up to 14 TeV. 
			
			Inside the accelerator, two high-energy particle beams travel close to the speed of light before they are made to collide. To collide two beams of equally charged particles requires opposite magnet dipole fields in both beams. The LHC is therefore designed as a proton-proton collider with separate magnet fields and vacuum chambers in the main arcs and with common sections only at the insertion regions where the experimental detectors are located. 
		
			\subsection{LHC layout}
				The basic layout of the LHC \cite{LHC_DESIGN_2004} follows the LEP tunnel geometry (Figure \ref{fig:LHC_layout}). The LHC has eight arcs and straight sections. Each straight section is approximately 528 m long and can serve as an experimental or utility insertion. The two high luminosity experimental insertions are located at diametrically opposite straight sections: the {ATLAS} experiment is located at point 1 and the CMS experiment at point 5. Two more experimental insertions are located at point 2 and point 8 which also contain the injection systems for Beam 1 and Beam 2, respectively. The remaining four straight sections do not have beam crossings. Insertion 3 and 7 each contain two collimation systems. Insertion 4 contains two RF systems: one independent system for each LHC beam. The straight section at point 6 contains the beam dump insertion where the two beams are vertically extracted from the machine using a combination of horizontally deflecting fast-pulsed ("kicker") magnets and vertically-deflecting double steel septum magnets. Each beam features an independent abort system.
				
				\begin{figure}[H]
					\centering
					\subfloat[][\emph{LHC dipole}]{\includegraphics[width=.57\textwidth]{thesis_images/m_dipole.jpeg}} \quad
					\subfloat[][\emph{LHC quadpole}]{\includegraphics[width=.4\textwidth]{thesis_images/m_quad.png}} 
					\caption{LHC's superconducting magnets}
					\label{fig:magnets}
				\end{figure}
				
				The LHC beams are controlled by superconducting magnets, which have a working temperature of 1.9 K. There are two kinds of superconducting magnets (Figure \ref{fig:magnets}):
				\begin{itemize}
					\item the superconducting dipole magnets, which thanks to a 8.33 T magnetic field drive protons along the ring (circular orbit);
					\item superconducting quadrupole magnets, which keep the beams focused.
				\end{itemize} 
				
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.5\textwidth]{thesis_images/LHC_layout.png}
					\caption{LHC structure}
					\label{fig:LHC_layout}
				\end{figure}
			
			\subsection{CERN accelerator complex}
				The aim of the CERN accelerator complex (Figure \ref{fig:CERN_complex}) is to accelerate the particle beam, which could be inserted into LHC ring. The accelerator complex a succession of machines that accelerate particles to increasingly higher energies. Each machine boosts the energy of a beam of particles before injecting it into the next machine in the sequence. In the Large Hadron Collider (LHC), which is the last element in this chain, particle beams are accelerated up to the record energy of \textcolor{red}{6.5 TeV} per beam.
				
				Linear accelerator 4 (Linac4) became the source of proton beams for the CERN accelerator complex in 2020. It accelerates negative hydrogen ions to 160 MeV to prepare them to enter the Proton Synchrotron Booster (PSB). The ions are stripped of their two electrons during injection from Linac4 into the PSB, leaving only protons. These are accelerated to 2 GeV for injection into the Proton Synchrotron (PS), which pushes the beam up to 26 GeV. Protons are then sent to the Super Proton Synchrotron (SPS), where they are accelerated up to 450 GeV. The protons are finally transferred to the two beam pipes of the LHC. The beam in one pipe circulates clockwise while the beam in the other pipe circulates anticlockwise. It takes 4 minutes and 20 seconds to fill each LHC ring, and 20 minutes for the protons to reach their maximum energy of \textcolor{red}{6.5 TeV}. Beams circulate for many hours inside the LHC beam pipes under normal operating conditions. The two beams are brought into collision inside four detectors (ALICE, ATLAS, CMS and LHCb) where the total energy at the collision point is equal to \textcolor{red}{13 TeV}. In addition to accelerating protons, the accelerator complex can also accelerate lead ions.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.7\textwidth]{thesis_images/CERN_complex.png}
					\caption{CERN accelerator complex}
					\label{fig:CERN_complex}
				\end{figure}
			
			\subsection{Proton-proton collisions}
				The number of events per second generated in the LHC collisions is given by:
				$$
				N_{i} = L\sigma_{i}
				$$
				where $\sigma_{i}$ is the cross section for the process under study and $L$ the instantaneous machine luminosity. $L$ depends only on the beam parameters and can be written for a Gaussian beam distribution as: 
				$$
				L = \frac{N_b^2n_bf_{rev}\gamma_r}{4\pi\epsilon_n\beta^*}F
				$$
				where $N_b$ is the number of particles per bunch, $n_b$ the number of bunches per beam, $f_{rev}$ the revolution frequency, $\gamma_r$ the relativistic gamma factor, $\epsilon_n$ the normalized transverse beam emittance, $\beta^*$ the beta function at the collision point and $F$ the geometric luminosity reduction factor due to the crossing angle at the IP. The luminosity had various values during ATLAS activity as shown in the Figure \ref{fig:insta lum}.
				
				\begin{figure}
					\centering
					\includegraphics[width=0.5\textheight]{thesis_images/peakLumiByFill.pdf}
					\caption{The peak instantaneous luminosity delivered to ATLAS during stable beams for pp collisions at 13 TeV centre-of-mass energy per fill 2018}
					\label{fig:insta lum}
				\end{figure}
				
				For the nominal value of the luminosity $10^{34}$cm$^{-2}$s$^{-1}$ the total inelastic proton-proton cross-section is about 80 mb at $\sqrt{s} = 14$ TeV. Therefore, the event rate $R$, defined as the number of events produced per second by the pp interactions, is expected to be:
				$$
				R = \sigma L = 80 mb\times10^{34}cm^{-2}s^{-1} \simeq 10^{9}s^{-1}.
				$$ 
				The number of events for each process is related to luminosity, it is proportional to the integrated luminosity shown in Figure \ref{fig:Integreted Luminosity}.
				
				\begin{figure}
					\centering
					\includegraphics[width=0.5\textheight]{thesis_images/intlumivsyear.pdf}
					\caption{Luminosity measured in inverse femtobarn versus time for 2011-2018 (p-p data only)}
					\label{fig:Integreted Luminosity}
				\end{figure}
				
				There are two types of pp collisions:
				\begin{itemize}
					\item \textbf{Soft collisions}: 
					they are the most of the collisions and they are large-distance collisions between the two incoming protons. They are called "soft" because the momentum transfer of the interaction is small. Due to this feature, particle scattering at large angle is suppressed and so, after collisions, particles have a large longitudinal momentum, but small transverse momentum ($<p_T>\simeq500$ MeV) relative to the beam line. The final states
					arising from such interactions are called minimum bias events.
					\item \textbf{Hard collisions}:
					monochromatic proton beams can be seen as beams of partons (quarks and gluons) with a wide band
					of energy. Occasionally, head-on collisions occur between two partons of the incoming protons. These are interactions at small distances, and therefore are characterised by large
					momentum transfers ("hard scattering"). In this case, particles in the final state can be produced at large angles with respect to the beam line (high $p_T$) and massive particles can be created.
					These are the interesting physics events at a collider but they are, however, rare compared to the soft interactions.
					
					In the hard-scattering interactions of quarks and gluons at a hadron collider, the effective centre-of-
					mass energy of the interaction ($\sqrt{\hat{s}}$) is smaller than the centre-of-mass energy of the machine ($\sqrt{s}$) and is given by: 
					$$ 
					\sqrt{\hat{s}} = \sqrt{x_ax_bs}
					$$
					where $x_a$ and $x_b$ are the fractions of the proton momentum carried by the two colliding partons. If $x_a \simeq x_b$, then the above relation becomes
					$$ 
					\sqrt{\hat{s}} \simeq x\sqrt{s}
					$$
					Therefore, in order to produce a particle of mass 100 GeV, two quarks (or gluons) which carry only 1\% of the proton momentum are needed ($x \sim 0.01$), whereas a particle of mass 5 TeV can only be produced if two partons with $x \sim 0.35$ interact. The momentum distributions of quarks and gluons inside the proton are called \textit{parton distribution functions}. % da fuonte p219
				\end{itemize}
			
	
	
		\section{ATLAS}
			The LHC produces an enormous amount of events having unprecedented high energy and luminosity. Inside the LHC, bunches of up to 10$^{11}$ protons
			will collide 40 million times per second to provide 14 TeV proton-proton collisions at a design luminosity of $10^{34} cm^{-2} s^{-1}$
			
			The high interaction rates, radiation doses, particle multiplicities and energies, as well as the requirements for precision measurements have set new standards for the design of particle detectors. Two general purpose detectors, ATLAS (A Toroidal LHC ApparatuS) and CMS (Compact Muon Solenoid) have been built for probing $pp$ collisions.
	
			ATLAS \cite{ATLAS_DESIGN_2008} investigates a wide range of physics, from the search for the Higgs boson to extra dimensions and particles that could make up dark matter. It is a forward-backward symmetric detector with respect to the interaction point and it composed by six different detecting subsystems arranged in layers around the collision point, which record the paths, momentum, and energy of the particles, allowing them to be individually identified. A complex magnet system bends the paths of charged particles so that their momenta can be measured. The interactions in the ATLAS detectors create an enormous flow of data. To digest the data, ATLAS uses an advanced “trigger” system to tell the detector which events to record and which to ignore. Complex data-acquisition and computing systems are then used to analyse the recorded collision events. At 46 m long, 25 m high and 25 m wide, the 7000-tonne ATLAS detector is the largest volume particle detector ever constructed.
			
			\subsection{Coordinate system}
				ATLAS has a specific coordinate system and nomenclature. The interaction point, which is where the beams collide and the ATLAS centre, is defined as the origin of the coordinate system. The beam direction defines the $x$-$y$-$z$ coordinates: the $z$-axis is along this direction whereas the $x$-$y$ plane is transverse to it. The positive $x$-axis is defined as pointing from the interaction point to the centre of the LHC ring and the positive $y$-axis is defined as pointing upwards. The azimuthal angle $\phi$ is measured as usual around the beam axis, and the polar angle $\theta$ is the angle from the beam axis. The pseudorapidity is defined as $\eta = - \ln \tan(\theta/2)$. The transverse momentum $p_T$, the transverse energy $E_T$, and the missing transverse energy $E_T^{miss}$ are defined in the $x$-$y$ plane.
				The distance $\Delta R$ in the pseudorapidity-azimuthal angle space is defined as $\Delta R = \sqrt{\Delta\eta^2 + \Delta\phi^2}$.
				
			\subsection{Magnet system}
				ATLAS features a unique hybrid system of four large superconducting magnets and this magnet structure has driven the design of the rest of the detector. The overall dimensions of the magnet system are 26 m in length and 20 m in diameter, with a stored energy of 1.6 GJ.
				\begin{itemize}
					\item a solenoid ("central solenoid" CS), which is aligned on the beam axis and provides a $2$ T axial magnetic field for the inner detector,  while minimising the radiative thickness in front of the barrel electromagnetic calorimeter;
					\item a  barrel  toroid and  two  end-cap  toroids, which  produce  a toroidal magnetic field of approximately $0.5$ T and $1$ T for the muon detectors in the central and end-cap regions, respectively.
				\end{itemize}
			
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.4\textheight]{thesis_images/magnet_system_atlas.png}
					\caption{ATLAS magnet system}
				\end{figure}
			
			\subsection{Inner Detector}
				The inner-most layer of ATLAS detector is the Inner Detector (ID). Hermetic and robust pattern recognition, excellent momentum resolution, both primary and secondary vertex measurements for charged tracks within the pseudorapidity range $|\eta|$ < 2.5 and electron identification over $|\eta|$ < 2.0 up to energy of about 150 GeV are provided by the ID. These features are achieved with high-resolution detectors at the inner radii and  with continuous tracking elements at the outer radii, all contained in the CS which provides a nominal magnetic field of 2 T. The ID has a cylindrical 6.2 m long shape with a diameter of 2.1 m.
				
				The ID is composed by three independent but complementary sub-detectors \cite{ID_report}:
				\begin{itemize}
					\item \textbf{Pixel Detector}
						\begin{figure}
							\centering
							\includegraphics[width=0.3\textheight]{thesis_images/id1_struct.jpeg}
							\includegraphics[width=0.3\textheight]{thesis_images/id2_struct.jpg}
							\caption{Inner Detector structure}
						\end{figure}
						The pixel detector is designed to provide a very high-granularity, high-precision set of measurements as close to the interaction point as possible. The system consists of three barrels at average radii of $\sim$ 4 cm, 11 cm, and 14 cm, and four disks
						on each side, between radii of 11 and 20 cm, which complete the angular coverage. The system provides three of the precision measurements over the full acceptance, and determines the impact parameter resolution and the ability of the Inner Detector to find short-lived particles such as b-quarks and $\tau$-leptons. So, due to its high spatial resolution and 3-dimensional space-point measurement the Pixel Detector has a key-role in reconstruction of charged particle tracks. The 4-Layer Pixel Detector is crucial in the reconstruction of primary and secondary vertices which is essential for example in the photon conversions reconstruction. The single point resolution is 12 $\mu m$ in the $R\cdot\phi$ and 66-77 $\mu m$ in $z$ direction.
					\item \textbf{Semi-Conductor Tracker}
						The SCT system is designed to provide four precision measurements per track in the intermediate radial range, contributing to the measurement of momentum, impact parameter and vertex position, as well as providing good pattern recognition by the use of high granularity. The SCT covers the radial region from 30 to 52 cm. The barrel SCT uses four layers of silicon microstrip detectors to provide precision points in the $R\cdot\phi$ and $z$ coordinates. Nine annular disks on each end of the barrel form the “endcaps”. The single point resolution is 16 $\mu m$ in the $R\cdot\phi$ and 580 $\mu m$ in $z$ direction.
					\item \textbf{Transition Radiation Tracer} 
						 The TRT is the outmost of the three tracking subsystems of the ATLAS Inner Detector. It is a straw-tube tracker. When a charged particle traverses the TRT, it ionises the gas inside the straws. The resulting free electrons drift towards the wire, where they are amplified and read out. The spaces between the straws are filled with polymer fibres (barrel) and foils (endcaps) to create transition radiation, which may be emitted by highly relativistic charged particles as they traverse a material boundary.   This effect depends on the relativistic factor $\gamma=E/m$ and is strongest for electrons, so it could be a discriminating factor. This design  makes the TRT complementary to  the silicon-based tracking devices:  the intrinsic single-point resolution of 170 $\mu$m is larger than that of the silicon trackers, but this is compensated by the large number of hits per track (typically more than 30). 
						
					
				\end{itemize}
			
			\subsection{Calorimetry}
				The calorimeters are designed to absorb most of the particles coming from a collision, forcing them to deposit all of their energy and stop within the detector. \cite{ATLAS_DESIGN_2008} ATLAS calorimeters (Figure \ref{fig:all_calo_struct}) consist of layers of an "absorbing” high-density material that stops incoming particles, interleaved with layers of an "active medium, such as solid lead-glass or liquid argon, that measures their energy. Electromagnetic (EM) calorimeters measure the energy of $e^{\pm}$ and $\gamma$ as they interact with matter, whereas hadronic calorimeters sample the energy of hadrons as they interact with atomic nuclei. Calorimeters are designed to stop most known particles except muons and neutrinos.
				\begin{figure}
					\centering
					\includegraphics[width=0.8\textwidth]{thesis_images/atlas_calo.jpg}
					\caption{ATLAS Calorimeter structure}
					\label{fig:all_calo_struct}
				\end{figure}

				
				The components of the ATLAS calorimetry system are:
				\begin{itemize}
					\item \textbf{LAr Electromagnetic Calorimeter}: the ATLAS EM calorimeter \cite{LArCalo_report} (Figure \ref{fig:calo struct}) is a lead-liquid argon (LAr) sampling detector with accordion-shaped electrodes and lead absorber plates over its full coverage. The calorimeter is divided into a Barrel part ($|\eta|<1.475$) and two End-Caps. Each End-Cap is divided into two coaxial wheels: an outer wheel and an inner wheel covering, respectively, $1.375<|\eta|<2.5$ and $2.5<|\eta|<3.2$. The absorber lead thickness is constant over large areas. The argon gap thickness is constant in the Barrel but changing with the radius in the End-Cap. In the range $|\eta|<1.8$, the calorimeter is preceded by a presampler to recover the energy lost in the upstream material (cryostat, super-conducting coil, inner detector, etc.). The typical energy resolution is:
					$$ 
					\frac{\sigma(E)}{E} = \frac{10\%}{\sqrt{E}} \oplus 0.7\%
					$$
					
					\begin{figure}[h!]
						\centering
						\includegraphics[width=.6\textwidth]{thesis_images/calo_struct.jpg}
						\caption{The EM calorimeter accordion geometry}
						\label{fig:calo struct}
					\end{figure}
					\begin{table}[h!]
						\centering
						\begin{tabular}{cc}
							\toprule[1.5pt]
							\textbf{Layer} & \textbf{Granularity ($\Delta\eta \times \Delta\phi$)} \\
							\midrule
							Presampler & $0.025 \times 0.1$ \\
							Layer 1 & $0.0031 \times 0.1$ \\
							Layer 2 & $0.025 \times 0.0245$ \\
							Layer 3 & $0.05 \times 0.245$ \\
							\bottomrule[1.5pt]
						\end{tabular}
						\caption{Granularity of different layers of the EM calorimeter}\label{tab:granularity}
						
					\end{table}

					\item \textbf{Hadronic calorimeter}: the Hadronic calorimeter \cite{LArCalo_report} is composed by two parts. The first one is the tile calorimeter, which is placed directly outside the EM calorimeter envelope. Its barrel covers the region $|\eta|$ < 1.0, and its two extended barrels the range 0.8 < $|\eta|$ < 1.7. It is a sampling calorimeter using steel as the absorber and scintillating tiles as the active material. The second part of hadronic calorimeter is the the Hadronic End-cap Calorimeter (HEC), which consists of two independent wheels per end-cap, located directly behind the end-cap electromagnetic calorimeter. It is an LAr sampling calorimeter using copper and tungsten as passive material. It provides full coverage for $1.5<|\eta|<3.2$. The typical resolution for jets is:
					$$ 
					\frac{\sigma(E)}{E} = \frac{50\%}{\sqrt{E}} \oplus 3\%
					$$
					
					\item \textbf{Foward Calorimeter}: the Foward Calorimeter \cite{LArCalo_report} covers the high-pseudorapidity range $3.1 < |\eta| < 4.9$. It is divided in three regions in the $z$-direction that uses argon as active material: the first (made of copper) is optimised for electromagnetic showers containment, the other two (made of tungsten) are used for hadronic jets measurements. The typical resolution for jets is:
					$$ 
					\frac{\sigma(E)}{E} = \frac{100\%}{\sqrt{E}} \oplus 10\%
					$$
				\end{itemize}
			\subsection{Muon system}
				The last layer of the ATLAS detector is the muon system	\cite{muon_report} (Figure \ref{fig:muon detector}). It was designed to serve two purposes: an independent muon trigger and high quality stand-alone muon reconstruction over a wide range in transverse momentum, pseudo-rapidity ($|\eta|$ < 2.4 trigger, $|\eta|$ < 2.7 momentum) and azimuthal angle. This is achieved by the use of a large toroidal magnet system together with trigger and high precision tracking chambers. Through the use of a toroidal magnetic field of 0.3 T, the MS deflects charged particles and measures the muon momentum with high precision. Since calorimeters are designed to fully stop EM and hadronic showers, only muons reach the MS. Over most of the $\eta$-range, a precision measurement of the track coordinates in the principal bending direction of the magnetic field is provided by Monitored Drift Tubes (MDT’s) and by and Cathode Strip Chambers (CSCs) in the end-cap region. The trigger system covers the pseudorapidity range $|\eta|$ < 2.4. Resistive Plate Chambers (RPC’s) are used in the barrel and Thin Gap Chambers (TGC’s) in the end-cap regions. The trigger chambers for the muon spectrometer serve a threefold purpose: provide bunch-crossing identification, provide well-defined $p_T$ thresholds, and measure the muon coordinate in the direction orthogonal to that determined by the precision-tracking chambers.
				
				The typical energy resolution is resolution is 2-3\% to 10 GeV and goes up to 10\% to 1 TeV.
				\begin{figure}
					\centering
					\includegraphics[width=0.65\textwidth]{thesis_images/3D-muon.jpg}
					\caption{ATLAS Muon system structure}
					\label{fig:muon detector}
				\end{figure}
			
			\subsection{Trigger system}
				ATLAS is designed to observe up to 1.7 billion proton-proton collisions per second, with a combined data volume of more than 60 million megabytes per second. However, only some of these events will contain interesting characteristics that might lead to new discoveries. To reduce the flow of data to manageable levels, ATLAS uses a specialised two-level online event selection system, the Trigger System \cite{trigger}, which selects events with distinguishing characteristics that make them interesting for physics analyses. The Trigger System selects approximately 1000 of the 1.7 billion collisions that occur each second in the centre of the ATLAS detector. 
				
				\begin{figure}
					\centering
					\subfloat[]{\includegraphics[width=.49\textwidth]{thesis_images/L1.png}}					
					\subfloat[]{\includegraphics[width=.49\textwidth]{thesis_images/HLT.png}} 
					\caption{L1 (a) and HLT (b) physics trigger rates grouped by trigger signatures as a function of theluminosity block number, in a fill taken in July 2016 with a peak instantaneous luminosity of $1.2 \times 10^{34}$cm$^{-2}$s$^{-1}$}
					\label{fig:Trigger system}
				\end{figure}
				
				The two independent levels (Figure \ref{fig:Trigger system}) are, a hardware-based first level (L1) and a software-based high level trigger (HLT). 
				\begin{itemize}
					\item The L1 trigger is implemented in fast custom-made electronics and runs with a fixed latency of 2.5$\mu$s. L1 reduces the event rate from the LHC interaction rate of 40 MHz to $\approx$ 100 kHz. Up to 512 decision items are built, based on Regions of Interest (RoI) in $\eta/\varphi$ retrieved from the muon (L1Muon) and calorimeter (L1Calo) systems. The L1 trigger decision is formed by the Central Trigger Processor (CTP). The L1Topo trigger, which is a system introduced for Run-2, performs selections based on geometric or kinematic association between trigger objects received from L1Calo or L1Muon. 
					
					\item In the HLT, offline-like reconstruction algorithms run in a large farm of $\approx$ 40.000 processor cores and a decision is formed typically within 300 ms. The HLT is a software trigger providing typically 2500 independent trigger chains. These are sequences of offline-like algorithms executed within the L1 RoIs. Furthermore, full-event reconstruction is possible at the HLT. Events accepted by the HLT are written into different data streams to be used for physics analysis, trigger level analysis, monitoring or detector calibration. Depending on the datastream, the full event or only partial event information is written out, allowing for higher rates without consuming a significant amount of the available bandwidth.
				\end{itemize}
		
	
	\chapter{Photon reconstruction}\label{chapter:ph_reco}
		Event with photons in the final state are important signatures for many physics analyses envisaged at the LHC. For example, in the Higgs studies, the $H \to \gamma\gamma$ channel, despite the $\gamma\gamma$ branching fraction is subdominant, the analyses in this final state profit from an excellent mass resolution and manageable backgrounds. These events reconstruction mainly exploits data coming from the electromagnetic calorimeter (clusters) and the Inner Detector (ID) systems (tracks) \cite{Aad_2019}:
		\begin{itemize}
			\item a converted photon is a cluster matched to either two tracks forming a conversion vertex, or one track with the signature of an electron track without hits in the innermost pixel layer;
			\item an unconverted photon is a cluster matched to neither an electron track nor a conversion vertex.
		\end{itemize}
		About 20\% of photons at low $|\eta|$ convert in the ID, and up to about 65\% convert at $|\eta| \simeq$ 2.3. 
		
		\begin{figure}
			\centering
			\includegraphics[width=.8\textwidth]{thesis_images/alg_flow.png} 
			\caption{Algorithm flow diagram for the electron and photon reconstruction.}
			\label{fig:alg ph}
		\end{figure}
		
		The electron, defined as an object consisting of a cluster built from energy deposits in the calorimeter (supercluster) and a matched track (or tracks), shares a few reconstruction and selection steps with photons. The reconstruction of photons with $|\eta|$ < 2.5 proceeds as shown in Figure \ref{fig:alg ph}. The algorithm first prepares the tracks and clusters it will use. It selects clusters of energy deposits measured in topologically connected EM and hadronic calorimeter cells \cite{topo_cluster}, denoted topo-clusters, reconstructed as described in the next section. These clusters are matched to ID tracks, which are re-fitted accounting for bremsstrahlung. The algorithm also builds conversion vertices and matches them to the selected topo-clusters. The electron and photon supercluster-building steps then run separately using the matched clusters as input. After applying initial position corrections and energy calibrations to the resulting superclusters, the supercluster-building algorithm matches tracks to the electron superclusters and conversion vertices to the photon superclusters. The electron and photon objects to be used for analyses are then built, their energies are calibrated, and discriminating variables used to separate electrons or photons from background are added.
		
		\section{The topo-cluster reconstruction}\label{section:topo cluster}
			The topo-cluster reconstruction algorithm \cite{topo_cluster} use a set of noise thresholds is commonly known as "4-2-0" topo-cluster reconstruction:
			\begin{itemize}
				\item The algorithm begins by forming proto-clusters in the EM and hadronic calorimeters using as starting point cells with a significance $|\zeta_{cell}^{EM}| \geq 4$ where 
				$$
				\zeta_{cell}^{EM} = \frac{E_{cell}^{EM}}{\sigma_{cell}^{EM}}
				$$		
				$E_{cell}^{EM}$ is the energy cell at the EM scale and $\sigma_{cell}^{EM}$ is the expected cell noise, which includes the known electronic noise and an estimate of the pile-up noise corresponding to the average instantaneous luminosity expected for Run 2. In this initial stage, cells from the presampler and the L1 are excluded from initiating proto-clusters, as this suppresses the formation of noise clusters.
				\item Then the proto-clusters collect neighbouring cells with significance $|\zeta_{cell}^{EM}| \geq 2$ merging with the corresponding \textit{seed} cell. These neighbour cells become seeds cell in the next iteration of the algorithm until there are no cells near a proto-cluster with significance $|\zeta_{cell}^{EM}| \geq 2$.
				\item A crown of nearest-neighbour cells is added to the cluster independently on their energy, $|\zeta_{cell}^{EM}| \geq 0$.
			\end{itemize}
			
			After the "4-2-0" topo-cluster reconstruction, a check about the local maximum is performed. A local maximum is a cell with $E^{EM}_{cell} > 500$ MeV, at least four neighbours, and when none of the neighbours has a larger signal. Proto-clusters with two or more local maxima are split into separate clusters.
		
		\section{Track reconstruction}
			After the topo-cluster creation, the photon reconstruction algorithm proceeds to the next step: track finding. Track finding is one of the most challenging tasks in reconstructing events from proton-proton  collisions  recorded  by  the  ATLAS  detector. The process consists in finding a track from hits recorded in the ID. The ID track reconstruction consists of several sequences with different strategies and the main sequence is referred to as inside-out track finding \cite{ID_track1}: 
			\begin{itemize}
				\item \textbf{Space point formation}: the transformation of clusters in the silicon detectors into 3D space points is the initial step of the track reconstruction.  Clusters are formed by finding connected cells in the pixel and strip detectors. The pixel detector provides a local two-dimensional cluster measurement as one space-point, whereas in the SCT, clusters from both stereo views of a strip layer must be combined to obtain a three-dimensional measurement.

				\item \textbf{Space point seeded track finding}: from the set of the three space-points in the silicon-detector layers created in the previous step the track seeds are formed. There could be different types of track seed depending of the nature of the three space-points: 
				\begin{itemize}
					\item PPP seed is built from space points in the pixel detector only;
					\item SSS seed is built from space points in the strip detector only;
					\item PSS and PPS are mixed setup;
				\end{itemize}
				The number of all possible seeds is then reduced applying initial selections and avoiding the extensive use of space points in multiple seeds. These selected seed are then the input to a track finding algorithm that uses a combinatorial Kalman filter technique \cite{Kalman} and aims to complete the track candidates within the silicon detector.
				
				\item \textbf{Ambiguity solving}: before the TRT extension, the track candidates pass through a process which eliminates track candidates from random hit combinations (often referred to as "fakes") or track duplicates, which can be identified by measurements that are shared with other track candidates. This process is called ambiguity solving and it is based on a \textit{track scoring}: tracks with good features, such as unique measurements and good fit quality, are given a positive score, whereas tracks with holes (missing measurements) or shared measurements with other track candidates are penalised. After the scoring, the hits shared between tracks are assigned to the track with higer score, while the remaining track is refitted without the formerly shared hit, and then scored again.
				
				\item \textbf{TRT extension}: the tracks used for the TRT extension must successfully pass the ambiguity solving stage and be within the coverage of the TRT detector. A successful TRT extension increases the momentum resolution significantly.
			\end{itemize}
			
		\section{Track-cluster matching and photon conversion reconstruction}
			Once clusters and tracks have been reconstructed, the next step is their matching \cite{Aad_2019}. Firstly, region-of-interest (ROIs) are created using fixed-size clusters in the calorimeter that have a longitudinal and lateral shower profile compatible with that of an EM shower. A ROI consists of an area of $\Delta R = 0.3$ \footnote{The distance between two points is defined as $\Delta R = \sqrt{(\Delta\eta)^2+(\Delta\phi)^2}$} around the cluster barycenter. Tracks are considered lossely matched to the cluster if they intersect the cluster ROI.
			
			
			Track candidates are then fitted with the global $\chi^2$ fitter \cite{ChiQuadro}. The loosely matched, re-fitted tracks are then matched to the EM topo-clusters following as described in \cite{Aad_2019}. If multiple tracks are matched to a cluster, they are ranked in order to choose the best one. Tracks with hits in the pixel detector are preferred, then tracks with hits in the SCT but not in the pixel detector. Another discriminating feature is $\Delta R$ between the track and cluster barycenter in the second layer coordinates. Tracks with a better $\Delta R$ are preferred. If there are small $\Delta R$ differences, the track with more pixel hits is preferred, giving an extra weight to a hit in the innermost layer. So, the highest-ranked track is used to define the reconstructed electron properties.
			
			Tracks loosely matched to fixed-size clusters serve as input to the reconstruction of the conversion vertex. Both tracks with silicon hits and tracks reconstructed only in the TRT are used for the conversion reconstruction. Two-track conversion vertices are reconstructed from two opposite-charge tracks forming a vertex consistent with that of a massless particle, while single-track vertices are essentially tracks without hits in the innermost sensitive layers. To increase the converted-photon purity, the tracks used to build conversion vertices must have a high probability to be electron tracks as determined by the TRT.
			
			The conversion vertices are then matched to the EM topo-clusters. If the conversion vertex has tracks with silicon hits, a conversion vertex is considered matched if, after extrapolation, the tracks match the cluster to within $|\Delta\eta| < 0.05$ and $|\Delta\phi| < 0.05$. If the conversion vertex is made of only TRT tracks, then if the first track is in the TRT barrel, a match requires $|\Delta\eta| < 0.35$ and $|\Delta\phi| < 0.02$, and if the first track is in the TRT endcap, a match requires $|\Delta\eta| < 0.2$ and $|\Delta\phi| < 0.02$. If there are multiple conversion vertices matched to a cluster, double-track conversions with two silicon tracks are preferred over other double-track conversions, followed by single-track conversions. Within each category, the vertex with the smallest conversion radius is preferred.
		
		\section{Supercluster reconstruction}
			The reconstruction of electron and photon superclusters proceeds independently, each in two stages:
			\begin{enumerate}
				\item EM topo-clusters are tested for use as seed cluster candidates, which form the basis of superclusters;
				\item EM topo-clusters near the seed candidates are identified as satellite cluster candidates, which may emerge from bremsstrahlung radiation or topo-cluster splitting. The satellite cluster must satisfy selection criteria, described below.
			\end{enumerate}
			Therefore, the steps to build superclusters are:
			\begin{figure}
				\centering
				\includegraphics[width=0.6\textheight]{thesis_images/super_cluster.png}
				\caption{Diagram of the superclustering algorithm for electrons and photons. Seed clusters are shown in red, satellite clusters in blue.}
				\label{fig:super_cl}
			\end{figure}
			\begin{itemize}
				\item The initial list of EM topo-clusters, created in the Section \ref{section:topo cluster}, is sorted according to descending $E_T$, calculated using the EM energy. Each cluster is tested in the sort order for use as seed cluster. For a cluster to become an electron supercluster seed, it must have $E_T \geq$ 1 GeV must be matched to a track with $\geq$ 4 hits in the silicon tracking detectors. For
				photon reconstruction, a cluster must have $E_T \geq$ 1.5 GeV with no requirement made on any track or conversion vertex matching. A cluster cannot be used as a seed cluster if it has already been added as a satellite cluster to another seed cluster.
				\item Once a cluster has been selected as a seed, the algorithm attempts to find satellite clusters (Figure \ref{fig:super_cl}). A $\Delta\eta\times\Delta\phi = 0.075 \times 0.125$ window is setted around the seed cluster barycentre and every cluster which falls in this window is considered as a satellite. For electrons, a cluster is also considered a satellite if it is within a window of $\Delta\eta\times\Delta\phi = 0.125 \times 0.300$ around the seed cluster barycentre, and its "best-matched" track is also the best-matched track for the seed cluster. For photons with conversion vertices made up only of tracks containing silicon hits, a cluster is added as a satellite if its best-matched (electron) track belongs to the conversion vertex matched to the seed cluster. These steps rely on tracking information to discriminate distant radiative photons or conversion electrons from pile-up noise or other unrelated clusters. Seed cluster with its satellite clusters is called \textit{supercluster}.
				\item The final step in the
				supercluster-building algorithm is to assign calorimeter cells to a given supercluster. Only cells from the presampler and the first three LAr calorimeter layers are considered, except in the transition region of $1.4 < |\eta| < 1.6$, where the energy measured in the scintillator between the calorimeter cryostats is also added. To limit the superclusters’ sensitivity to pile-up noise, the size of each constituent topo-cluster is restricted to a maximal width of 0.075 or 0.125 in the $\eta$ direction in the barrel or endcap region, respectively. Because the magnetic field in the ID is parallel to the beam-line, interactions between the electron or photon and detector material generally cause the EM shower to spread in the $\phi$ direction, so the restriction in $\eta$ still generally allows the electron or photon energy to be captured. No restriction is applied in the $\phi$-direction.
			\end{itemize}
			
		\section{Electron and photon ambiguity resolution}
			Since electron and photon superclusters are built independently and thus a given seed cluster can produce both, in such case it is necessary to apply an ambiguity resolution algorithm (Figure \ref{fig:e_ph_alg}). This algorithm purpose is that if a particular object can be easily identified only as a photon (a cluster with no good track attached) or only as an electron (a cluster with a good track attached and no good photon conversion vertex), then only a photon or an electron object is created for analysis; otherwise, both an electron and a photon object are created. Furthermore, these cases are marked explicitly as ambiguous, allowing the final classification of these objects to be determined based upon the specific requirements of each analysis.
			\begin{figure}
				\centering
				\includegraphics[width=.5\textheight]{thesis_images/el_ph_analisi.png}
				\caption{Flowchart showing the logic of the ambiguity resolution for particles initially reconstructed both as electrons and photons.}
				\label{fig:e_ph_alg}
			\end{figure}
		
		\section{Calibration}\label{section:Calib}
			The energy calibration \cite{calibration} covers the region $|\eta|$ < 2.47, which corresponds to the acceptance of the ID and the highly segmented EM calorimeter. Here are reported the various steps of the procedure to calibrate the energy response of electrons and photons from the energy of a cluster of cells in the EM:
			\begin{itemize}
				\item The first step is the estimation of the energy of the electron or photon from the energy deposits in the calorimeter. A multivariate regression algorithm used for this estimation is trained on samples of simulated events. the properties of the shower development are used to optimize the energy resolution and to minimize the impact of material in front of the calorimeter.
				\item Studies of muon energy deposits and electron showers are used in order to adjust the relative energy scales of the different layers of the EM calorimeter, obtaining the correct extrapolation of the energy calibration to the full energy range of electrons and photons. It is applied as a correction to the data before the estimation of the energy of the electron or photon.
				\item The next step is the correction for residual local non-uniformities in the calorimeter response affecting the data.
				\item Using a large sample of $Z$ boson decays to electron-positron pairs, the adjustment of the overall energy scale in the data is performed. At the same time, a correction to account for the difference in energy resolution between data and simulation is derived, and applied to the simulation. These correction factors are assumed to be universal for electrons and photons.
				\item the last step is to check the results comparing data and simulation with independent samples: $J/\psi \rightarrow ee$ decays probe the energy response for low-energy electrons. Radiative $Z$ boson decays are used to check the energy response for photons.
			\end{itemize}
		
		\section{Identification}\label{section:Ident}
			The identification is fundamental step in the photon reconstruction process. Many aspects of the ATLAS physics program, Standard Model measurements (including Higgs boson) and new physics searches require excellent electron and photon identification capabilities \cite{el_id}\cite{ph_id}. The $\gamma$ and $e^\pm$ identification processes use different discriminating variables and techniques:
			\begin{itemize}
				\item The identification of prompt photons and the rejection of backgrounds, mostly coming from photons from hadron decays, relies on the high granularity of the ATLAS calorimeter.
				\item  Electron identification is based on a likelihood (LH) discrimination to separate isolated electron candidates from candidates originating from photon conversions, hadron misidentification and heavy flavor decays.
			\end{itemize}
			
			\subsection{Photon Identification}
				The photon identification criteria \cite{Aad_2019} are designed to efficiently select prompt, isolated photons and reject backgrounds from hadronic jets. The photon identification is constructed from one-dimensional selection criteria, or a \textit{cut-based selection}, using the shower shape variables described in Table \ref{tab:parameters}. The variables using the EM first layer play a particularly important role in rejecting $\pi^0$ decays into two highly collimated photons. 
				
				The primary identification selection is labelled as \textit{Tight}, with less restrictive selections called \textit{Medium} and \textit{Loose}. The \textit{Loose} identification uses the $R_{had}$, $R_{had_1}$, $R_{\eta}$ and $w_{\eta_2}$ shower shape variables. The \textit{Medium} selection adds a loose cut on $E_{ratio}$. Because the reconstruction of photons in the ATLAS trigger system does not differentiate between converted and unconverted photons, the \textit{Loose} and \textit{Medium} identification criteria are the same for converted and unconverted photons. The \textit{Tight} identification criteria are designed to select a subset of the photon candidates passing the \textit{Medium} criteria. Because the shower shapes vary due to the geometry of the calorimeter, the cut-based selection of \textit{Loose}, \textit{Medium} and \textit{Tight} are optimized separately in bins of $|\eta|$. The \textit{Tight} identification presented here is also optimized in separate bins of $E_T$. The \textit{Tight} identification is performed separately for converted and unconverted photons. The shower shapes of converted photons differ from unconverted photons due to the opening angle of the $e^+e^-$ conversion pair, which is amplified by the magnetic field, and from the additional interaction of the conversion pair with the material upstream of the calorimeters.
				
				\vspace{-1.83cm}
				
				\begin{center}
					\begin{table}
						\resizebox{\textwidth}{!}{
							\begin{tabular}{lp{12cm}cc}
								\toprule[1.5pt]
								\textbf{Type} & \textbf{Description} & \textbf{Name} &  \textbf{Usage}\\
								\midrule
								Hadronic leakage
								& - Ratio of $E_T$ in the first layer of the hadronic calorimeter to $E_T$ of the EM calorimeter (in the pseudorapidity range $|\eta| < 0.8$ or $|\eta| > 1.37$) & $R_{had1}$ & $e/\gamma$ \\ 
								& - Ratio of $E_T$ in the first layer of the hadronic calorimeter to $E_T$ of the EM calorimeter (in the pseudorapidity range $0.8 < |\eta| < 1.37$) & $R_{had}$ & $e/\gamma$ \\
								\midrule
								Layer 3 of the EM calorimeter & Ratio of the energy in the back layer to the total energy in the EM calorimeter. This variable is only used below 100 GeV because it is known to be inefficient at high energies. & $f_3$ & $e$ \\
								\midrule
								Layer 2 of the EM calorimeter
								& - Ratio of the sum of the energies of the cells contained in a $3\times7\times\eta\times\phi$ rectangle (measured in cell units) to the sum of the cell energies in a $7\times7$ rectangle, both centred around the most energetic cell & $R_\eta$ & $e/\gamma$ \\
								& - Lateral shower width $\sqrt{(\sum_{i}E_i\eta_i^2)/(\sum_{i}E_i) - ((\sum_{i}E_i\eta_i)/(\sum_{i}E_i))^2}$ ,where
								$E_i$ is the EM calorimeter energy and $\eta_i$ is the pseudorapidity of cell i and the sum is calculated within a window of 3 $\times$ 5 cells & $w_{\eta_2}$ & $e/\gamma$ \\
								& - Ratio of the sum of the energies of the cells contained in a $3\times3\times\eta\times\phi$ rectangle (measured in cell units) to the sum of the cell energies in a $3\times7$ rectangle, both centred around the most energetic cell & $R_\phi$ & $e/\gamma$ \\
								\midrule
								Layer 1 of the EM calorimeter
								& - Total lateral shower width, $\sqrt{(\sum_{i}E_i(i-i_{max})^2)/(\sum_{i}E_i)}$ where $i$ runs over all strips in a window of $\Delta\eta\approx0.0625$ and $i_{max}$ is the index of the highest-energy strip & $w_{stot}$ & $e/\gamma$ \\
								& - Lateral shower width, $\sqrt{(\sum_{i}E_i(i-i_{max})^2)/(\sum_{i}E_i)}$, where i runs over all cells in a window of 3 cells around the highest-energy cell & $w_{s3}$ & $\gamma$ \\
								& - Energy fraction outside core of three central cells, within seven cells & $f_{side}$ & $\gamma$ \\
								& - Difference between the energy of the cell associated with the second maximum, and the energy reconstructed in the cell with the smallest value found between the first and second maxima & $\Delta E_s$ & $\gamma$ \\
								& - Ratio of the energy difference between the maximum energy deposit and the energy deposit in a secondary maximum in the cluster to the sum of these energies & $E_{ratio}$ & $e/\gamma$ \\
								& - Ratio of the energy measured in the first layer of the electromagnetic calorimeter to the total energy of the EM cluster & $f_1$ & $e/\gamma$ \\
								\midrule
								Track conditions
								& - Number of hits in the innermost pixel layer & $n_{innermost}$ & $e$ \\
								& - Number of hits in the Pixel Detector & $n_{Pixel}$ & $e$ \\
								& - Number of total hits in the pixel and SCT detectors & $n_{Si}$ & $e$ \\
								& - Transverse impact parameter  respect to the beam-line & $d_0$ & $e$ \\
								& - Significance of transverse impact parameter defined as the ratio of $d_0$ and its uncertainty & $|d_0/\sigma_{d_0}|$ & $e$ \\
								& - Momentum lost by the track between the perigee and the last measurement point divided by the momentum at perigee & $\Delta p/p$ & $e$ \\ 
								& - Likelihood probability based on transition radiation in the TRT & eProbabilityHT  & $e$ \\
								\midrule
								Track-cluster matching
								& - $\Delta \eta$ between the cluster position in first layer of the EM calorimeter and the extrapolated track & $\Delta \eta_1$ & $e$ \\
								& - $\Delta \phi$ between the cluster position in the second layer of the EM calorimeter and the momentum-rescaled track, extrapolated from the perigee, times the charge $q$ & $\Delta \phi_res$ & $e$ \\
								& - Ratio of the cluster energy to the measured track momentum & $E/p$ & $e$ \\
								\bottomrule[1.5pt]
						\end{tabular}}
						\caption{Discriminating variables used for $e^{\pm}$ and $\gamma$ identification \cite{Aad_2019}}
						\label{tab:parameters}
					\end{table}
				\end{center} 
				\begin{center}
					\begin{table}
						\centering
						\begin{tabular}{lp{6cm}ccc}
							\toprule[1.5pt]
							\textbf{Category} & \textbf{Description} & \textbf{Name} & \textbf{\textit{loose}} & \textbf{\textit{tight}}\\
							\midrule
							Acceptance & $\eta<2.37$, with $1.37<|\eta|<1.52$ excluded & - & $\ast$ & $\ast$ \\
							\midrule
							\multirow[t]{2}{*}{Hadronic leakage} 
							& {- Ratio of $E_T$ in the first layer of the hadronic calorimeter to $E_T$ of the EM calorimeter (used over range $|\eta| < 0.8$ or $|\eta| > 1.37$)} & $R_{had1}$ & $\ast$ & $\ast$  \\ 
							& - Ratio of $E_T$ in the first layer of the hadronic calorimeter to $E_T$ of the EM calorimeter (in the pseudorapidity range $0.8 < |\eta| < 1.37$) & $R_{had}$  & $\ast$ & $\ast$ \\
							\midrule
							\multirow[t]{3}{*}{EM Middle layer}
							& - Ratio of 3$\times$7 $\eta\times\phi$ to 7$\times$7 cell energies & $R_{\eta}$ & $\ast$ & $\ast$ \\
							& - Lateral width of the shower & $w_{\eta2}$ & $\ast$ & $\ast$ \\
							& - Ratio of 3$\times$3 $\eta\times\phi$ to 7$\times$7 cell energies & $R_{\phi}$ &  & $\ast$ \\
							\midrule
							\multirow[t]{5}{*}{EM Strip layer}
							& - Shower width calculated from three strips around the strip with maximum energy deposit & $w_{s3}$ &  & $\ast$ \\
							& - Total lateral shower width & $w_{stot}$ &  & $\ast$ \\
							& - Energy outside the core of the three central strips but within seven strips divided by energy within the three central strips & $F_{side}$ &  & $\ast$ \\
							& - Difference between the energy associated with the second maximum in strip layer(L1) and the energy reconstructed in the strip with the minimum value found between the first and second maxima & $\Delta E$ &  & $\ast$ \\
							& - Ratio of the energy difference associated with the largest and second largest energy deposits to sum of these energies & $E_{ratio}$ &  & $\ast$ \\
							\bottomrule
						\end{tabular}
						\caption{Discriminating variables used for \textit{loose} and \textit{tight} photon identification \cite{ph_id}}
						\label{tab:ph parameters}
					\end{table}
				\end{center}
						
		\section{Isolation}
			The last step of the photon reconstruction is the isolation, which fulfils the need to further reduce the backgrounds. The isolation criterium uses variables from the tracking and from the calorimeter \cite{isolation}.
			
			The activity near leptons and photons can be quantified from the tracks of nearby charged particles, or from energy deposits in the calorimeters, leading to two classes of isolation variables \cite{Aad_2019}:
			\begin{itemize}
				\item The raw calorimeter isolation($E_{T,raw}^{isol}$) is built by summing the transverse energy of positive-energy topological clusters (at EM scale) whose barycentre falls within a cone centred around the electron or photon cluster barycentre. The raw calorimeter isolation includes the EM particle energy ($E_{T,core}$), which is subtracted by removing the energy of the EM calorimeter cells contained in a $\Delta\eta \times \Delta\varphi = 5 \times 7$ (in EM-middle-layer units) rectangular
				cluster around the barycentre of the EM particle cluster. Other corrections are done to account for the pile-up contribution. %(using MC samples)%
				
				The fully corrected calorimeter isolation variable is finally computed as:
				$$ 
				E_{T}^{coneXX} = E_{T,raw}^{isolXX} - E_{T}^{core} - E_{T,leakage}(E_T,\eta,\Delta R) -E_{T,pile-up}(\eta,\Delta R)
				$$
				where XX refers to the size of the employed cone (a cone size of $\Delta$R = 0.2 is used for the electron or photon working point, whereas
				cone size of $\Delta$R = 0.4 is only used for photon working points) and the $E_{T,leakage}$ term refers to a correction for the $e/\gamma$ candidate energy leaking in the isolation cone.
				\item The track isolation variable ($p_T^{coneXX}$) is computed by summing the transverse momentum of selected tracks within a cone centred around the electron track or the photon cluster direction (excluding tracks matched to the electron or converted photon)\cite{El ph isol}. Since for electrons produced in the decay of high-momentum heavy particles can be very close to the electron direction, the track isolation for electrons is defined with a variable cone size:
				$$
				\Delta R = min\Bigl({\frac{10}{p_T[GeV]},\Delta R_{max}}\Bigr)
				$$
				where $\Delta R_{max}$ is the maximum cone size (typically 0.2).
				
				The tracks considered are required to have $p_T >$ 1 GeV and $|\eta| <$ 2.5, at least seven silicon (Pixel + SCT) hits, at most one shared hit, at most two silicon holes and at most one pixel hole. In addition, for electron isolation, the tracks are required to have a loose vertex association, i.e. the track was used in the primary vertex fit, or it was not used in any vertex fit but satisfies $|\Delta z_0|sin{\theta} < 3$ mm, where $|\Delta z_0|$ is the longitudinal impact parameter relative to the chosen primary vertex; for photon
				isolation, all selected tracks satisfying $|\Delta z_0|sin{\theta} < 3$ mm are used.
			\end{itemize}
			
			\subsection{Photon isolation criteria and efficiency measurements}
			Three photon isolation operating points are defined using requirements on the calorimeter and track isolation variables, as summarized in Table \ref{tab:ph_crit}. The photon isolation efficiency is studied in two main signatures: radiative $Z$ decays (valid for 10 < $E_T$ < 100 GeV) and inclusive photons (used in the 25 GeV < $E_T$ < $\sim$1.5 TeV range).
			
			\begin{center}
				\small
				\begin{table}[htbp]
					\resizebox{\textwidth}{!}{%
						\begin{tabular}{lcc}
							\toprule[1.5pt]
							\textbf{Working point} & \textbf{Calorimeter isolation} & \textbf{Track isolation} \\
							\midrule
							Loose		   & $E_T^{cone20} < 0.065 \times E_T$ 			  &  $p_T^{cone20}/E_t < 0.05$ \\
							Tight		   & $E_T^{cone40} < 0.022 \times E_T + 2.45\,GeV$ &  $p_T^{cone20}/p_t < 0.06$ \\
							TightCaloOnly  & $E_T^{cone40} < 0.022 \times E_T + 2.45\,GeV$ &  						   \\
							\bottomrule[1.5pt]
					\end{tabular}}
					\caption{Definition of the photon isolation working points.}
					\label{tab:ph_crit} 
				\end{table}
			\end{center}

	\chapter{Statistical techniques}
		In particle physics experiments one often searches for processes that have been predicted but not yet seen, such as production of a Higgs boson. 
		Typically, there are two forms for searches for new physics \cite{Higgs_statistic}:
		\begin{itemize}
			\item in the first, the total number of events is counted, which includes backgrounds and possible new physical events;
			\item in the second form, we consider the distribution of the so-called invariant mass of the particles formed in high-energy collisions. When compared with known background physics, new physics is expected to result in excess events with invariant mass near that of the new physical particle.
		\end{itemize}
		Either type of search can be formalized statistically as a hypothesis test: in the first form as a contaminated Poisson count and in the second as a search for a bump above a background distribution. In either case, the statistical search may involve one of several outcomes:
		\begin{enumerate}
			\item the conclusion that the data are inconsistent with the null hypothesis of the known background physics but consistent with the hypothesized new physics, resulting in the sought-after discovery of a new physical particle;
			\item the conclusion that the data are inconsistent with the hypothesized new physics;
			\item an upper limit on a possible signal strength generated by the new physics;
			\item a determination that the experiment is not sensitive enough to distinguish between new physics and the background;
		\end{enumerate}
	
		\section{Formalism of a search as a statistical test}
 			The general procedure, used to search for a new phenomenon in the context of a frequentist statistical test, starts defining the null hypothesis, $H_0$, as describing only known processes, here designated as background. This is to be tested against the alternative $H_1$, which includes both background as well as the sought-after signal \cite{Statistic}. In order to quantify the level of agreement of the observed data with a given hypothesis $H$, the $p$-value is computed. The $p$-value is the probability, under assumption of $H$, of finding data of equal or greater incompatibility with the predictions of $H$. The measure of incompatibility can be based, for example, on the number of events found in designated regions of certain distributions or on the corresponding likelihood ratio for signal and background. One can regard the hypothesis as excluded if its $p$-value is observed below a specified threshold.
 			
 			In particle physics one usually converts the $p$-value into an equivalent significance, $Z$, defined such that a Gaussian distributed variable found $Z$ standard deviations above its mean has an upper-tail probability equal to $p$. That is,
 			\begin{equation}\label{eq:Z}
 				Z = \Phi^{-1}(1-p)
 			\end{equation}
 			where $\Phi^{-1}$ is the quantile (inverse of the cumulative distribution) of the standard Gaussian. In the particle physics community, the background hypothesis with a significance of at least $Z$ = 5 ($p = 2.87\times10^{-7}$) is considered rejected and this is assumed as an appropriate level to constitute a discovery. For purposes of excluding a signal hypothesis, a threshold $p$-value of 0.05 (95\% confidence level) is often used, which corresponds to $Z$ = 1.64.
 			
 			A widely used procedure to establish discovery (or exclusion) in particle physics is based on a frequentist significance test using a likelihood ratio as a test statistic. In addition to parameters of interest such as the rate (cross section) of the signal process, the signal and background models will contain in general \textit{nuisance parameters} whose values are not taken as known \textit{a priori} but rather must be fitted from the data. In an experiment for each event in the signal sample a variable $x$ is measured and it is used to construct a histogram $n = (n_1,\dots, n_N)$. The expectation value of $n_i$ can be written as
 			\begin{equation}
 				E[n_i] = \mu s_i + b_i
 			\end{equation}
 			where the mean number of entries in the $i$-th bin from signal and background are
 			\begin{equation}
 				s_i = s_{tot} \int_{i-bin} f_s(x;\boldsymbol{\theta_s}) dx
 			\end{equation}
 			\begin{equation}
 				b_i = b_{tot} \int_{i-bin} f_b(x;\boldsymbol{\theta_b}) dx
 			\end{equation}
 			$\mu$ is the strength of the signal process, with $\mu$ = 0 corresponding to the background-only hypothesis and $\mu$ = 1 being the nominal signal hypothesis. The functions $f_s(x;\boldsymbol{\theta_s})$ and $f_b(x;\boldsymbol{\theta_b})$ are the probability density functions (pdfs) of the variable $x$ for signal and background events, and $\boldsymbol{\theta_s}$ and $\boldsymbol{\theta_b}$ represent parameters that characterize the shapes of pdfs. $s_{tot}$ and $b_{tot}$ are the total mean numbers of signal and background events. $s_{tot}$ is fixed to the value predicted by the nominal signal model.
 			
 			In addition to the measured histogram \textbf{n} further subsidiary measurements are made in order to help constrain the nuisance parameters. Using a control sample, where one expects mainly background events, an other histogram $\textbf{m} = (m_1;\dots;m_M)$ is created, measuring the chosen variable. The expectation value of $m_i$ can be written as
 			\begin{equation}
 				E[m_i] = u_i(\boldsymbol{\theta})  
 			\end{equation}
 			where the $u_i$ are calculable quantities depending on the parameters $\boldsymbol{\theta} = (\boldsymbol{\theta}_b,\boldsymbol{\theta}_s,b_{tot})$. One often constructs this measurement so as to provide information on the background normalization parameter $b_{tot}$ and also possibly on the signal and background shape parameters.
 			
 			The likelihood function is the product of Poisson probabilities for all bins:
 			\begin{equation}
 				L(\mu,\boldsymbol{\theta}) = \prod_{j=1}^{N}\frac{(\mu s_j + b_j)^{n_j}}{n_j!}e^{-(\mu s_j + b_j)} \prod_{k=1}^{M}\frac{u_k^{m_k}}{m_k!}e^{-u_k}
 			\end{equation}
 			
 			To test a hypothesized value of $\mu$, the profile likelihood ratio is considered
 			\begin{equation}\label{eq:ML}
 				\lambda(\mu) = \frac{L(\mu,\hatsrm{\boldsymbol{\theta}})}{L(\hat{\mu},\hat{\boldsymbol{\theta}})}
 			\end{equation}
 			where $\hatsrm{\boldsymbol{\theta}}$ is the value of $\boldsymbol{\theta}$ that maximizes $L$ for the specified $\mu$. It is the conditional maximum-likelihood (ML) estimator of $\boldsymbol{\theta}$ (and thus is a function of $\mu$). The denominator of Eq. \ref{eq:ML} is the maximized (unconditional) likelihood function, so $\hat{\mu}$ and $\hat{\boldsymbol{\theta}}$ are their ML estimators.
 		
 		\section{Test statistic}
 			The likelihood-ratio test \cite{Statistic} is the basis for formal detection and, under the unified approach, the construction of upper limits and intervals. %Considering the null hypothesis, $H_0$: $\mu = \mu_0$, where $\mu_0$ is typically zero and the alternative hypothesis $H_{\mu}$: $\mu>0$, it is defined 
 			From the definition of $\lambda(\mu)$ in Eq. \ref{eq:ML}, one can see that $0 \leq \lambda \leq 1$, with $\lambda$ near 1 implying good agreement between the data and the hypothesized value of $\mu$. Equivalently it is convenient to use the statistic:
 			\begin{equation}\label{eq:t_mu}
 				t_{\mu} = -2\ln\lambda(\mu)
 			\end{equation}
 			as the basis of a statistical test. Higher values of $t_{\mu}$ thus correspond to increasing incompatibility between the data and $\mu$. To measure discrepancy between the data and the hypothesis, the $p$-value is calculated
 			\begin{equation}\label{eq:p_mu}
 				p_{\mu} = \int_{t_{\mu,obs}}^{\infty} f(t_{\mu}|\mu) dt_{\mu}
 			\end{equation}
 			where $t_{\mu,obs}$ is the value of the statistic $t_{\mu}$ observed from the data and $f(t_{\mu}|\mu)$ denotes the pdf of $t_{\mu}$ under the assumption of the signal strength $\mu$. The relation between the $p$-value and the observed $t_{\mu}$ and also with the significance $Z$ are illustrated in Figure \ref{fig:p_mu_Z}
 			\begin{figure}
 				\centering
 				\subfloat[]{\includegraphics[width=.49\textwidth,height=.245\textheight]{thesis_images/t_p_mu.png}}					
 				\subfloat[]{\includegraphics[width=.49\textwidth,height=.245\textheight]{thesis_images/Z_p_mu.png}} 
 				\caption{(a) Illustration of the relation between the $p$-value obtained from an observed value of the test statistic $t_{\mu}$.\\ (b) The standard normal distribution $\varphi(x)=(1/\sqrt{2\pi})\exp(-x^2/2)$ showing the relation between the significance $Z$ and the $p$-value.}
 				\label{fig:p_mu_Z}
 			\end{figure}
 			\vspace{-3cm}
 			Specific statistical tests are created starting from Eq. \ref{eq:t_mu} and used in specific cases:
 			\begin{itemize}
 				\item \textbf{Test statistic $\tilde{t}_\mu$ for $\mu\geq0$}: It is often assumed that the presence of a new signal can only increase the mean event rate beyond what is expected from background alone. Therefore, the signal process necessarily has $\mu\geq0$, and to take this into account, an alternative test statistic called $\tilde{t}_\mu$ is defined
 				\begin{equation}\label{eq:t_mu_tilde}
 					\tilde{t}_\mu = -2\ln\tilde{\lambda}(\mu) = 
 					\begin{cases}
 						-2\ln\frac{L(\mu,\hatsrm{\boldsymbol{\theta}}(\mu))}{L(0,\hatsrm{\boldsymbol{\theta}}(0))} & \hat{\mu}<0 \\
 						-2\ln\frac{L(\mu,\hatsrm{\boldsymbol{\theta}}(\mu))}{L(\hat{\mu},\hat{\boldsymbol{\theta}})} & \hat{\mu}\geq0 \\
 					\end{cases}
 				\end{equation}
 				Here $\hatsrm{\boldsymbol{\theta}}(0)$ and $\hatsrm{\boldsymbol{\theta}}(\mu$) refer to the conditional ML estimators of $\boldsymbol{\theta}$ given a strength parameter of $0$ or $\mu$, respectively. As was done with the statistic $t_\mu$, one can quantify the level of disagreement between the data and the hypothesized value of $\mu$ with the $p$-value, just as in Eq. \ref{eq:p_mu}.
 				
 				\item  \textbf{Test statistic $q_0$ for discovery of a positive signal}: An important special case of the statistic $\tilde{t}_\mu$ described above is used to test $\mu = 0$ in a class of model where it is assumed $\mu \geq 0$. Rejecting the $\mu=0$ hypothesis effectively leads to the discovery of a new signal. This statistical test is called $q_0=\tilde{t}_0$
 				\begin{equation}\label{eq:q_0}
 					q_0 = 
 					\begin{cases}
 						-2\ln\lambda(0) & \hat{\mu}\geq0 \\
 						0 & \hat{\mu}<0 \\
 					\end{cases}
 				\end{equation}
 				If the data fluctuate such that one finds fewer events than even predicted by background processes alone, then $\hat{\mu}<0$ and one has $q_0 = 0$. As the event yield increases above the expected background (increasing $\hat{\mu}$), one finds increasingly large values of $q_0$, corresponding to an increasing level of incompatibility between the data and the $\mu=0$ hypothesis. To quantify the level of disagreement between the data and the hypothesis of $\mu=0$ using the observed value of $q_0$ the $p$-value is computed
 				\begin{equation}\label{eq:p_0}
 					p_0 = \int_{q_{0,obs}}^{\infty} f(q_0|0)\ dq_0
 				\end{equation}
 				It is common to plot the local significance as a function of the Higgs mass, $m_H$ (Figure \ref{fig:p_0}).
 				\begin{figure}
 					\centering
 					\includegraphics[width=.6\textwidth]{thesis_images/p_0.png}			
 					\caption{Local significance as a function of the Higgs mass, $m_H$. Values are based on preliminary results from ATLAS and are compared with the expected local significance under the $\mu=1$ hypothesis.}
 					\label{fig:p_0}
 				\end{figure}
 			
 				\item \textbf{Test statistic $q_\mu$ for upper limits}: For purposes of establishing an upper limit on the strength parameter $\mu$, a test statistic $q_\mu$ is defined
 				\begin{equation}\label{eq:q_mu_limit}
 					q_\mu = 
 					\begin{cases}
 						-2\ln\lambda(\mu) & \hat{\mu}\leq\mu \\
 						0 & \hat{\mu}>\mu \\
 					\end{cases}
 				\end{equation}
 				The reason for setting $q_\mu=0$ for $\hat{\mu}>\mu$ is that when setting an upper limit, one would not regard data with $\hat{\mu}>\mu$ as representing less compatibility with $\mu$ than the data obtained, and therefore this is not taken as part of the rejection region of the test. From the definition of the test statistic one sees that higher values of $q_\mu$ represent greater incompatibility between the data and the hypothesized value of $\mu$. Then the level of agreement between the data and hypothesized $\mu$ can be quantified with $p$-value, which can be expressed as a significance using Eq. \ref{eq:Z}.
 				
 				In a search for a signal of unknown mass, the procedure of finding upper limit on $\mu$ for a specific value of the peak position (mass)(Figure \ref{fig:mu_limit}) would be repeated for all masses producing Figure \ref{fig:limits}, which shows the median upper limit at 95\% CL as a function of mass. The upper limit on $\mu$ at a confidence level CL$=1-\alpha$ is the value of $\mu$ for which the $p$-value is $p_\mu=\alpha$. The value $p_\mu = 0.05$ corresponds to the 95\% CL upper limit. The median (central blue line) and error bands ($\pm1\sigma$ in green, $\pm2\sigma$ in yellow) are obtained using Asimov datasets, which are dataset generated for a particular set of model parameters such that the maximum likelihood best-fit value of all those parameters are equal to their generated values \cite{Asimov}. 
 				\begin{figure}
 					\centering
 					\includegraphics[width=.6\textwidth]{thesis_images/mu_limit.png}			
 					\caption{Distribution of the upper limit on $\mu$ at 95\% CL, assuming data corresponding to the background-only hypothesis}
 					\label{fig:mu_limit}
 				\end{figure}
	 			\begin{figure}[!h]
	 				\centering
	 				\includegraphics[width=.6\textwidth]{thesis_images/limits.png}			
	 				\caption{CL$_S$ upper limits as a function of the Higgs mass, $m_H$, for ATLAS.}
	 				\label{fig:limits}
	 			\end{figure}
 			\end{itemize}
 			
 	\chapter{Medium mass $m_{\gamma\gamma}$ analysis}
 		As said before, this analysis focuses on the search for new spin-0 resonances in the [105,200] GeV diphoton invariant mass range, using data that have been collected by the ATLAS experiment during the Run2 of the LHC. A spin-0 resonant state, which is predicted by many models that include extensions to the Higgs sector \cite{BSM}, could provide clues about BSM physics. 	
 		
 		Since this analysis is in search of new physics, a model independent of the Standard Model is created. In order to have the most general model possible, only the gluon-gluon Fusion \textit{ggF} is considered in the analysis. This allows to avoid making any assumption on the production modes for the new resonances. The other production modes, which contribute to Higgs$_{125}$ SM production, are included as a signal yield systematic uncertainty. It is therefore possible to include and parameterise the ignorance of the relative importance of different production modes for additional scalar Higgs bosons. 
 		
 	 	Despite its small branching fraction (Figure \ref{fig: br}), thanks to its clean signature, the $H\to\gamma\gamma$ channel is an ideal channel for the discovery of a low-mass Higgs boson. The QCD di-photon production processes constitute the irreducible background component (Figure \ref{fig:irr bkg}), whereas the event composed by a $\gamma$ and a jet wrongly identified as $\gamma$ constitute the reducible one (Figure \ref{fig:rid bkg}). This last component can be suppressed by requiring events with good-quality photons. Therefore, the analysis is based on the selection of pairs of high-$p_T$ and isolated photons. In order to maximise the signal and background ratio an accurate study of these background components is essential.
 		\begin{figure}
 			\centering
 			\includegraphics[width=.5\textwidth]{thesis_images/br.jpg}
 			\caption{SM Higgs branching ratio}
 			\label{fig: br}
 		\end{figure}
	 	\begin{figure}
	 		\centering
	 		\subfloat[$H\rightarrow\gamma\gamma$ irreducible background]{\includegraphics[width=.7\textwidth]{thesis_images/irr_bkg.png}\label{fig:irr bkg}}\\				
	 		\subfloat[$H\rightarrow\gamma\gamma$ reducible background]{\includegraphics[width=.7\textwidth]{thesis_images/rid_bkg.png}\label{fig:rid bkg}}	
	 		\caption{$H\rightarrow\gamma\gamma$ backgrounds}
	 		\label{fig:bkgs}
	 	\end{figure}
 		Then, the selected events are classified into mutually exclusive categories designed to enhance the analysis sensitivity. The signal in each category is modelled using simulated MC samples of SM (spin0) Higgs bosons decaying into two photons as a function of the mass of the resonance. For background modelling, a smoothly falling function fitted to data is used.
 		
 		A combined maximum likelihood fit in all the categories is performed to investigate the presence of a signal by computing the compatibility of the observed data with the background-only hypothesis. Limits on the signal cross section will be placed if no statistically significant excess with respect to the background expectation will be observed.
 		
 	
 		\section{Data and simulation samples}
 			\subsection{Data}
 			
 			\subsection{Simulation samples}
 				In order to optimize the analysis selections and characterize the signal and backgrounds, simulated Monte Carlo (MC) events are used. No data recorded by the ATLAS experiment are used for these steps, so as to avoid any bias in the analysis. 
 				
 				To model the spin-0 resonance signals, MC samples were generated for a hypothetical resonance mass $m_X$ (110, 125, 130 and 140 GeV) assuming a decay width $\Gamma_X$ of 4 MeV to describe a hypothetical resonance in the narrow-width approximation (NWA). Different type of Standard Model Higgs production modes are generated. \textit{ggF},\textit{VBF}, \textit{VH}, \textit{t$\bar{t}$H} and \textit{b$\bar{b}$H} are generated using PowhegBoxv2 \cite{Pow_1,Pow_2,Pow_3,Pow_4}. The \textit{ggF} 
 				

 	
 		\section{Event selection}\label{section:selection}
 			As described in Chapter \ref{chapter:ph_reco}, photons are reconstructed from calorimeter energy deposits, which are created using a dynamical, topological cell-clustering algorithm \cite{topo_cluster}. In order to fall inside the region of the electromagnetic (EM) calorimeter with a finely segmented first layer, reconstructed photons must satisfy $|\eta| < 2.37$ , and outside the range $1.37 < |\eta| < 1.52$ corresponding to the transition region between the barrel and endcap EM calorimeters. To reduce the impact of the contamination of jets photon candidates must satisfy identification criteria based on calorimeter shower shape variables \cite{Aad_2019}.
 			
 			Once the photon events are reconstructed, they pass through the selection step. Events are selected by first requiring at least two photons satisfying the \textit{loose} identification preselection criteria. The two highest-$p_T$ preselected photons are designated as the candidates for the diphoton system. Then the two preselected photons are required to satisfy the tight identification criteria and the isolation selection. This selection of photons includes isolation requirements, which are based on both calorimeter and track features, to further suppress jets misidentified as photons. Then additional kinematic requirements are required on the photon $p_T$ relative to the diphoton invariant mass: the leading photon must have $p_T/m_{\gamma\gamma} > 0.3$ and and the subleading photon must satisfy $p_T/m_{\gamma\gamma} > 0.25$.
 			
 			The fiducial volume for the spin-0 interpretation is defined in order to mimic the selection at the reconstruction level. Therefore, at truth level, two photons with $|\eta|<2.37$, $p_T/m_{\gamma\gamma} > 0.3$ and $p_T/m_{\gamma\gamma} > 0.25$ for the leading and subleading photons, respectively, are required. A isolation cut to the two photons is also applied by requiring that the sum of $E_T$ of all the stable particles (except neutrinos) found within a $\Delta = 0.2$ cone around the photon direction, less than $0.065\cdot p_T$. 
 			
 			\textcolor{red}{Aggiungere magari tabella con tutti i numeri dei vari prod modes per le due selezioni, magari mettere in appendice}
 			
 			\begin{table}[tbp]
 				\centering
					\begin{tabular}{llcc}
						\toprule[1.5pt]
						Parameter									& Name						& Reconstruction Level	& Truth Level		\\
						\midrule
						$|\eta^{\gamma_1}|$							& $\eta$ cut				& $<2.37$ 				& $<2.37$			\\
						$|\eta^{\gamma_2}|$							& $\eta$ cut				& $<2.37$ 				& $<2.37$			\\
						$p_T^{\gamma_1}/m_{\gamma\gamma}$			& Scalar relative $p_T$ cut & > 0.3					& > 0.3    			\\
						$p_T^{\gamma_2}/m_{\gamma\gamma}$			& Scalar relative $p_T$ cut & > 0.25				& > 0.25			\\
						$E_{T}^{cone20\ \gamma_1}/p_T^{\gamma_1}$ 	& Isolation cut 			& < 0.065				& < 0.065 			\\
						$E_{T}^{cone20\ \gamma_2}/p_T^{\gamma_2}$ 	& Isolation cut 			& < 0.065				& < 0.065			\\
						$p_{T}^{cone20\ \gamma_1}/p_T^{\gamma_1}$ 	& Isolation cut 			& < 0.05				& X		 			\\
						$p_{T}^{cone20\ \gamma_2}/p_T^{\gamma_2}$ 	& Isolation cut 			& < 0.05				& X					\\
						\bottomrule[1.5pt]
					\end{tabular}
 				\caption{Selection cuts of reconstructed and fiducial events }
 			\end{table}
 			
 		\section{Categorisation}
 			The selected events, described in Section \ref{section:selection}, are classified into mutually exclusive categories. Their definition is a crucial step: an appropriate choice of categories makes it possible to maximise the signal over background (S/B) ratio and thus provide an analysis as sensitive to new spin-0 resonances as possible. In 2012, the commonly used categorization, which is based on photon conversion, $|\eta_{S2}|$ and $p_{Tt_{\gamma\gamma}}$, was used for the Higgs boson discovery with the Run1 data \cite{higgs_atlas}. It is not possible to apply this categorization in a model independent analysis, in which only \textit{ggF} production mode is used whereas the others are included as a systematic uncertain. Indeed, due to the different production modes $p_{Tt_{\gamma\gamma}}$ distribution, applying the di-photon system transverse momentum selections of the Run1 discovery categorization would produce unacceptable values of this uncertainty (\textcolor{red}{Approfondimento appendice?}). Therefore three different type of categorisation have been created by simplifying Run1 discovery one:
 			\begin{itemize}
 				\item \textbf{Incluisive}: the first one is the simplest type of categorisation, no cuts are applied (Figure \ref{fig:inclusive}). So, all selected events are included in the analysis in a single group. The \texttt{Inclusive} categorisation is used as benchmark for how much improvement stronger categorisations bring and whether it is worth using them. 
 				\item \textbf{catConvEta}: the \texttt{catConvEta} categorisation is the Run1 discovery one without the $p_{Tt_{\gamma\gamma}}$ selections. Therefore there are 6 different categories: events with two unconverted photons are separated into \textit{unconverted} central ($|\eta|<0.75$ for both candidates), \textit{unconverted} transition (at least one photon with $1.3<|\eta|<1.75$) and \textit{unconverted} rest (all other events), events with at least one converted photon are separated into \textit{converted} central ($|\eta|<0.75$ for both candidates), \textit{converted} transition (at least one photon with $1.3<|\eta|<1.75$) and \textit{converted} rest (all other events). Therefore the \texttt{catConvEta} categorisation is based on conversion and $\eta$ cuts (Figure \ref{fig:catConvEta}).
 				\item \textbf{catConv}: the \texttt{catConv} categorisation is a further simplification of the Higgs$_{125}$ discovery categories. It is composed only of the conversion cut, therefore there are only two categories: events with two unconverted photons and events with at least one converted photon (Figure \ref{fig:catConv}).
 			\end{itemize}
 			\begin{figure}[h!]
 				\centering
 				\subfloat[Inclusive categorisation]{\includegraphics[width=1.\textwidth]{thesis_images/inclusive_tree.pdf}\label{fig:inclusive}}\\
 				\subfloat[\texttt{catConvEta} categorisation]{\includegraphics[width=1.\textwidth]{thesis_images/catConvEta_tree.pdf}\label{fig:catConvEta}}\\				
 				\subfloat[\texttt{catConv} categorisation]{\includegraphics[width=1.\textwidth]{thesis_images/catConv_tree.pdf}\label{fig:catConv}}	
 				\caption{Different type of categorisation}
 				\label{fig:Cats}
 			\end{figure}
 		\section{Models}
 			The signal and backgrounds modelling process is completely led and determined by the specific $H\to\gamma\gamma$ channel $m_{\gamma\gamma}$ distribution. This distribution in each category is described by an extended probability density function (pdf) in which the signal and background shapes are analytic functions of $m_{\gamma\gamma}$. In this analysis, the analytic functions are defined over the [100,195] GeV di-photon invariant mass range, with a blind search in [110,170] GeV $m_X$ range. Then a simultaneous fit of these pdfs is applied to the $m_{\gamma\gamma}$ distributions in the categories. Systematic uncertainties related to yield, shape of signal and background modelling are incorporated into the likelihood model as nuisance parameters. 
 			
 			\subsection{Signal Modelling}
 				The signal shape is described by a Double-Sided Crystal Ball (DSCB) distribution: a gaussian core, which is useful to describe the mass of $H\to\gamma\gamma$ candidates, completed by power-law tails at lower and higher $m_{\gamma\gamma}$ values. To create an analysis sensitive to additional spin-0 resonances, the signal model will be function of $m_H$. Therefore, in the model, each DSCB parameter is inserted as a linear distribution of $m_H$:
 				\begin{equation}\label{eq:DSCB_par}
 					p^{DSCB} = A_p + B_p\cdot m_H
 				\end{equation}
 				where $p^{DSCB}$ is a DSCB parameter. The $A_p$ and $B_p$ for each DSCB parameters in each category are obtained by a simultaneous fit to four different resonance mass (110, 125, 130 and 140 GeV) \textit{ggF} MC samples.
 				\begin{figure}
 					\centering
 					\includegraphics[width=.8\textwidth]{thesis_images/PowhegPy8_NNLOPS_ggH125_catConvEta_no_fit.pdf}
 					\caption{Signal fit using a DSCB for the \texttt{Inclusive} categorisation}
 					\label{fig:sig_fit}
 				\end{figure}
 				
 				In each category $i$, the normalization of the signal pdf, called signal yield, is expressed as:
 				\begin{equation}\label{eq:signal_yield}
 					N^i = \sigma_{ggF}(m_H) \cdot A_X(m_H) \cdot Br_{\gamma\gamma}(m_H) \cdot \mathcal{L}_{Run2} \cdot C_X^i(m_H)
 				\end{equation}
 				where:
 				\begin{itemize}
 					\item $\sigma_{ggF}(m_H)$ is the cross-section value for the specific $m_H$ obtained by fitting the $\sigma$ values for different masses, showed in Figure \ref{fig:xs_fit}.
 					\item $A_X(m_H)$ is the acceptance value for the specific mass obtained from the linear fit of the four $A_X$ values for the four resonance masses showed in Figure \ref{fig:ax_fit}. The $A_X$ factor for each resonance mass is defined as the ratio of the number of events that pass the fiducial selection $n_{fid}$ and the total number of MC sample events $n_{all}$: $A_X = n_{fid}/n_{all}$.
 					\item $Br_{\gamma\gamma}(m_H)$ is the branching fraction value for the specific $m_H$ obtained by fitting the $Br_{\gamma\gamma}$ values for different masses, showed in Figure \ref{fig:br_fit}.  
 					\item $\mathcal{L}_{Run2}$ is the value of the luminosity of LHC Run2 and it is 140 fb$^{-1}$.
 					\item $C_X^i(m_H)$ for the specific mass is obtained from the linear fit of the four $C_X^i$ values for the four resonance masses showed in Figure \ref{fig:cx_fit}. $C_X^i$ is the ratio of the number of events that pass the selection after the reconstruction $n_{reco}$ and the number of those that pass the fiducial selection $n_{fid}$: $A_X = n_{reco}/n_{fid}$.. Therefore, the $C_X^i$ factors are calculated for each resonance mass and for $i$ category. It is the 
 				
 					\begin{figure}[h!]
 						\centering
 						\subfloat[$\sigma$ fit]{\includegraphics[width=.49\textwidth]{thesis_images/x_section_fit.pdf}\label{fig:xs_fit}}
 						\subfloat[$A_X$ fit]{\includegraphics[width=.49\textwidth]{thesis_images/ax_fit.pdf}\label{fig:ax_fit}}\\
 						\subfloat[$Br_{\gamma\gamma}$ fit]{\includegraphics[width=.49\textwidth]{thesis_images/br_fit.pdf}\label{fig:br_fit}}				
 						\subfloat[$C_X^i(m_H)$ fit for the \texttt{Inclusive} categorisation]{\includegraphics[width=.49\textwidth]{thesis_images/cx_fit_catConvEta_no.pdf}\label{fig:cx_fit}}	
 						\caption{Signal yield fit component}
 						\label{fig:signal_yiels}
 					\end{figure}
 				\end{itemize}
 				
 			\subsection{Non resonant background}
 				The non-resonant background in the $m_{\gamma\gamma}$ spectrum comes mainly from the non-resonant production of photon pairs ($\gamma\gamma$ events Figure \ref{fig:irr bkg}), smaller background components come from events containing a photon and a jet ($\gamma j$ events) and events with two jets ($jj$ events), where the jets are misidentified as photons (Figure \ref{fig:rid bkg}). Therefore, the background in each category is estimated from \texttt{Sherpa} di-photon MC samples by fitting the $m_{\gamma\gamma}$ spectrum in the mass range [100,195] GeV with a selected model with free parameters of shape and normalisation. The fitting range is set wider than the blind search range ([110,170] GeV $m_X$) in order to have enough events in the $m_{\gamma\gamma}$ side-bands to ensure a good description by the analytic form.
 				
 				\begin{figure}[h]
 					\centering
 					\includegraphics[width=.8\textwidth]{thesis_images/bkg_100_195GeV_fit_catConvEta_no.pdf}
 					\caption{Non-resonant background fit using the \ref{eq:bkg} function form for the \texttt{Inclusive} categorisation}
 					\label{fig:bkg_fit}
 				\end{figure}
 				The functional form used for the description of the non-resonant background in the $i$ category is (Figure \ref{fig:bkg_fit}):
 				\begin{equation}\label{eq:bkg}
 					f^i_{bkg}(m_{\gamma\gamma},N_{bkg}^i,a^i,b^i) = N_{bkg}^i\cdot\exp^{(a\cdot m_{\gamma\gamma}+b\cdot m_{\gamma\gamma}^2)}
 				\end{equation}
 				where $N_{bkg}^i$, which is the non-resonant background normalisation, $a^i$ and $b^i$ are free parameters.
 				
 			\subsection{Higgs Standard Model background}
 				Since the model must be SM independent in order to investigate the presence of new spin-0 resonances, the SM Higgs$_{125}$ is included as a resonant background (HSM). It is described by a DSCB fit to HSM sample, which is obtained by merging all SM Higgs$_{125}$ production MC samples generated with a resonance mass $m_H=125$ GeV (Figure \ref{fig:HSM_fit}) Therefore for the $i$ category:
 				\begin{itemize}
 					\item the DSCB fit is applied to $i$ category HSM sample;
 					\item all DSCB parameters are fixed at their value after the fit and inserted in the analysis;
 					\item the HSM background yield is obtained by counting the total number of events of $i$ category HSM sample;
 				\end{itemize}  
 				Therefore, the HSM model has no free parameter, but they can be shifted due to the systematic experimental uncertainties that are included into the analysis, described in the next Section \ref{section:syst}
 				\begin{table}
 					\centering
						\begin{tabular}{lc}
							\toprule[1.5pt]
							Cat &  HSM\_events \\
							\midrule
							Inclusive & 6232.25 \\
							\midrule
							catConvEta\_1 & 960.525 \\
							catConvEta\_2 & 1600.96 \\
							catConvEta\_3 & 466.921 \\
							catConvEta\_4 & 599.439 \\
							catConvEta\_5 & 1820.56 \\
							catConvEta\_6 & 787.967\\
							\midrule
							catConv\_1 & 3028.29 \\
							catConv\_2 & 3207.86 \\
							\bottomrule[1.5pt]
						\end{tabular}
 					\caption{\centering Number of SM Higgs$_{125}$ events for each category}
 				\end{table}
 				\begin{figure}[]
 					\centering
 					\includegraphics[width=.8\textwidth]{thesis_images/HSM_fit_catConvEta_no.pdf}
 					\caption{SM Higgs$_{125}$ resonant background fit using a DSCB for the \texttt{Inclusive} categorisation}
 					\label{fig:HSM_fit}
 				\end{figure}
 			
 			
 			
 		\subsection{Systematic uncertainties}\label{section:syst}
	\newpage
	\bibliography{biblio.bib}
	\bibliographystyle{unsrturl}
	\addcontentsline{toc}{chapter}{Bibliography}
	

\end{document}